# estimate the pose of target objects detected
import numpy as np
import json
import os
import ast
import cv2
from network.scripts.detector import Detector
# detector gets the bounding box and class label of the detected target(s) in an image

# list of target fruits and vegs types
# Make sure the names are the same as the ones used in your YOLO model
target_list = ['redapple', 'greenapple', 'orange', 'mango', 'capsicum']

def estimate_pose(camera_matrix, obj_info, robot_pose=[[0.0],[0.0],[0.0]], relative_pose_only=False):
    """
    function:
        estimate the pose of a target based on size and location of its bounding box and the corresponding robot pose
    input:
        camera_matrix: list, the intrinsic matrix computed from camera calibration (read from 'param/intrinsic.txt')
            |f_x, s,   c_x|
            |0,   f_y, c_y|
            |0,   0,   1  |
            (f_x, f_y): focal length in pixels
            (c_x, c_y): optical centre in pixels
            s: skew coefficient (should be 0 for PenguinPi)
        obj_info: list, an individual bounding box in an image (generated by get_bounding_box, [label,[x,y,width,height], conf])
        robot_pose: list, pose of robot corresponding to the image (read from 'lab_output/images.txt', [x,y,theta])
    output:
        target_pose: dict, prediction of target pose
    """
    # read in camera matrix (from camera calibration results)
    focal_length = camera_matrix[0][0]

    # there are 8 possible types of fruits and vegs
    ######### Replace with your codes #########
    # measure actual sizes of targets [width, depth, height] and update the dictionary of true target dimensions
    target_dimensions_dict = {'redapple': [0.077, 0.080, 0.090], 'greenapple': [0.081, 0.082, 0.077],
                              'orange': [0.075, 0.075, 0.068], 'mango': [0.113, 0.067, 0.055],
                              'capsicum': [0.075, 0.075, 0.091]}
    # target_dimensions_dict = {'redapple': [0.077, 0.080, 0.085], 'greenapple': [0.081, 0.082, 0.077],
    #                           'orange': [0.075, 0.075, 0.072], 'mango': [0.113, 0.067, 0.058],
    #                           'capsicum': [0.075, 0.075, 0.088]}
    #########

    # estimate target pose using bounding box and robot pose
    target_class = obj_info[0]     # get predicted target label of the box
    target_box = obj_info[1]       # get bounding box measures: [x,y,width,height]
    true_height = target_dimensions_dict[target_class][2]   # look up true height of by class label

    # compute pose of the target based on bounding box info, true object height, and robot's pose
    pixel_height = target_box[3]  # bbox height
    pixel_center = target_box[0]  # centre x
    distance = (true_height*1.12)/pixel_height * focal_length  # estimated distance between the object and the robot based on height
    # image size 640x480 pixels, 640/2=320
    x_shift = 640/2 - pixel_center              # x distance between bounding box centre and centreline in camera view
    theta = np.arctan(x_shift/focal_length)     # angle of object relative to the robot
    horizontal_relative_distance = distance * np.sin(theta)     # relative distance between robot and object on x axis
    vertical_relative_distance = distance * np.cos(theta)       # relative distance between robot and object on y axis
    relative_pose = {'y': vertical_relative_distance, 'x': horizontal_relative_distance}    # relative object location
    #ang = theta + robot_pose[2]     # angle of object in the world frame

    if relative_pose_only:
        return relative_pose
    else:
        # location of object in the world frame (transform robot frame to world frame using 3x3 homogenous matrix )
        target_pose = {'y': (robot_pose[1]+relative_pose['y']*np.sin(robot_pose[2])+relative_pose['x']*np.cos(robot_pose[2]))[0],
                       'x': (robot_pose[0]-relative_pose['x']*np.sin(robot_pose[2])+relative_pose['y']*np.cos(robot_pose[2]))[0]}

        return target_pose


def merge_estimations(target_pose_dict):
    """
    function:
        merge estimations of the same target
    input:
        target_pose_dict: dict, generated by estimate_pose 'label_occurence': {'y': 10.0, 'x': 5.0}
    output:
        target_est: dict, target pose estimations after merging
    """
    target_est = {}

    ######### Replace with your codes #########
    # TODO: replace it with a solution to merge the multiple occurrences of the same class type (e.g., by a distance threshold)
    target_pose_dict = target_pose_dict
    redapple_est, greenapple_est, orange_est, mango_est, capsicum_est = [], [], [], [], []
    num_per_target = 1 # max number of units per target type. We are only use 1 unit per fruit type
    # combine the estimations from multiple detector outputs
    for key in target_pose_dict:
        if key.startswith('redapple'):
            redapple_est.append(np.array(list(target_pose_dict[key].values()), dtype=float))
        elif key.startswith('greenapple'):
            greenapple_est.append(np.array(list(target_pose_dict[key].values()), dtype=float))
        elif key.startswith('orange'):
            orange_est.append(np.array(list(target_pose_dict[key].values()), dtype=float))
        elif key.startswith('mango'):
            mango_est.append(np.array(list(target_pose_dict[key].values()), dtype=float))
        elif key.startswith('capsicum'):
            capsicum_est.append(np.array(list(target_pose_dict[key].values()), dtype=float))
   
    ######### Replace with your codes #########
    # TODO: the operation below is the default solution, which simply takes the first estimation for each target type.
    # Replace it with a better merge solution.
    if len(redapple_est) > num_per_target:
        redapple_est_x_list = [arr[0] for arr in redapple_est]
        redapple_est_y_list = [arr[1] for arr in redapple_est]
        redapple_est[0][0] = np.median(redapple_est_x_list)
        redapple_est[0][1] = np.median(redapple_est_y_list)
    if len(greenapple_est) > num_per_target:
        greenapple_est_x_list = [arr[0] for arr in greenapple_est]
        greenapple_est_y_list = [arr[1] for arr in greenapple_est]
        greenapple_est[0][0] = np.median(greenapple_est_x_list)
        greenapple_est[0][1] = np.median(greenapple_est_y_list)
    if len(orange_est) > num_per_target:
        orange_est_x_list = [arr[0] for arr in orange_est]
        orange_est_y_list = [arr[1] for arr in orange_est]
        orange_est[0][0] = np.median(orange_est_x_list)
        orange_est[0][1] = np.median(orange_est_y_list)
    if len(mango_est) > num_per_target:
        mango_est_x_list = [arr[0] for arr in mango_est]
        mango_est_y_list = [arr[1] for arr in mango_est]
        mango_est[0][0] = np.median(mango_est_x_list)
        mango_est[0][1] = np.median(mango_est_y_list)
    if len(capsicum_est) > num_per_target:
        capsicum_est_x_list = [arr[0] for arr in capsicum_est]
        capsicum_est_y_list = [arr[1] for arr in capsicum_est]
        capsicum_est[0][0] = np.median(capsicum_est_x_list)
        capsicum_est[0][1] = np.median(capsicum_est_y_list)

    for i in range(num_per_target):
        try:
            target_est['redapple_'+str(i)] = {'x':redapple_est[i][1], 'y':redapple_est[i][0]}
        except:
            pass
        try:
            target_est['greenapple_'+str(i)] = {'x':greenapple_est[i][1], 'y':greenapple_est[i][0]}
        except:
            pass
        try:
            target_est['orange_'+str(i)] = {'x':orange_est[i][1], 'y':orange_est[i][0]}
        except:
            pass
        try:
            target_est['mango_'+str(i)] = {'x':mango_est[i][1], 'y':mango_est[i][0]}
        except:
            pass
        try:
            target_est['capsicum_'+str(i)] = {'x':capsicum_est[i][1], 'y':capsicum_est[i][0]}
        except:
            pass
    ###########################################
    return target_est


# main loop
if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))     # get current script directory (TargetPoseEst.py)

    # read in camera matrix
    fileK = f'{script_dir}/calibration/param/intrinsic.txt'
    camera_matrix = np.loadtxt(fileK, delimiter=',')

    # TODO:init YOLO model
    model_path = f'{script_dir}/network/scripts/model/model.best.pt'
    yolo = Detector(model_path)

    # create a dictionary of all the saved images with their corresponding robot pose
    image_poses = {}
    with open(f'{script_dir}/lab_output/images.txt') as fp:
        for line in fp.readlines():
            pose_dict = ast.literal_eval(line)
            image_poses[pose_dict['imgfname']] = pose_dict['pose']

    # estimate pose of targets in each image
    target_pose_dict = {} 
    detected_type_list = []
    for image_path in image_poses.keys():
        input_image = cv2.imread(image_path)
        bounding_boxes, bbox_img = yolo.detect_single_image(input_image)
        # detect_single_image = detect target(s) in an image
        # bounding_boxes: list of lists, box info [label,[x,y,width,height]] for all detected targets in image
        # bbox_img: image with bounding boxes and class labels drawn on
        # cv2.imshow('bbox', bbox_img)
        # cv2.waitKey(0)
        robot_pose = image_poses[image_path]

        for detection in bounding_boxes:
            # count the occurrence of each target type
            occurrence = detected_type_list.count(detection[0])
            #print(detection[0],occurrence)
            target_pose_dict[f'{detection[0]}_{occurrence}'] = estimate_pose(camera_matrix, detection, robot_pose)
            #print(target_pose_dict)
            detected_type_list.append(detection[0]) #detection[0] = label

    # merge the estimations of the targets so that there are at most 3 estimations of each target type
    target_est = {}
    target_est = merge_estimations(target_pose_dict)
    #print(target_est)
    # save target pose estimations
    with open(f'{script_dir}/lab_output/targets.txt', 'w') as fo:
        json.dump(target_est, fo)

    print('Estimations saved!')